{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating dataset\n",
    "face_cascade = cv2.CascadeClassifier('C:\\\\Users\\\\snsha\\\\Desktop\\\\openCV\\\\Face Haarcascade Classifier.xml')\n",
    "cap = cv2.VideoCapture(0)\n",
    "face_id = input(\"Enter id and press enter\")\n",
    "#directory = 'C:\\\\Users\\\\snsha\\\\Desktop\\\\openCV\\\\dataset'\n",
    "count = 0\n",
    "while True:\n",
    "   \n",
    "    check, frame = cap.read()\n",
    "    gray = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray,1.3,5)\n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(frame, (x,y), (x+w,y+h),(0,255,0),2)\n",
    "        count += 1\n",
    "        cv2.imwrite(\"dataset/User\" + str(face_id) + '-' + str(count) + \".jpg\", gray[y:y+h,x:x+w])\n",
    "        cv2.imshow(\"capture\",frame)\n",
    "        \n",
    "    key = cv2.waitKey(100) & 0xff\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "    elif count >= 80:\n",
    "        break\n",
    "#print(os.listdir(directory)) \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "        \n",
    "         \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#required libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing dataset \n",
    "datadir = \"C:\\\\Users\\\\snsha\\\\Desktop\\\\openCV\\\\dataset\"\n",
    "cat = [\"Shashank\",\"Nidhi\"]\n",
    "training_data = []\n",
    "def create_data():\n",
    "    for cate in cat:\n",
    "        path = os.path.join(datadir,cate) #creating path to the image folder\n",
    "        class_num = cat.index(cate)\n",
    "        for img in os.listdir(path):\n",
    "            img_array = cv2.imread(os.path.join(path,img)) #full path to the image\n",
    "            new_array = cv2.resize(img_array,(50,50))\n",
    "            training_data.append([new_array,class_num])\n",
    "create_data()\n",
    "random.shuffle(training_data) # shuffling the dataset \n",
    "X = []\n",
    "Y = []\n",
    "for features,labels in training_data:\n",
    "    X.append(features)\n",
    "    Y.append(labels)\n",
    "#saving the dataset as pickle file \n",
    "pickle_out = open(\"X.pickle\",\"wb\")\n",
    "pickle.dump(X, pickle_out)\n",
    "pickle_out.close()\n",
    "pickle_out = open(\"Y.pickle\",\"wb\")\n",
    "pickle.dump(Y, pickle_out)\n",
    "pickle_out.close()\n",
    "#slpitting images into test and train sets\n",
    "x_train_orig, x_test_orig, y_train_orig, y_test_orig = train_test_split(X, Y, test_size=0.15, random_state=1)\n",
    "\n",
    "x_train = np.array(x_train_orig)\n",
    "x_test = np.array(x_test_orig)\n",
    "y_train = np.array(y_train_orig)\n",
    "y_test = np.array(y_test_orig)\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#method to read the dataset\n",
    "#X = pickle.load(open(\"X.pickle\",\"rb\"))\n",
    "#print(X[0])\n",
    "#Y = pickle.load(open(\"Y.pickle\",\"rb\"))\n",
    "#print(Y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot encoding\n",
    "def convert_to_one_hot(labels, C):\n",
    "    C = tf.constant(C, name = \"C\")\n",
    "    one_hot_matrix = tf.one_hot(labels,C,axis = 0)\n",
    "    #sess = tf.compat.v1.Session\n",
    "    #one_hot = sess.run(one_hot_matrix)\n",
    "    #sess.close()\n",
    "    return one_hot_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training examples = 136\n",
      "number of test examples = 24\n",
      "X_train shape: (7500, 136)\n",
      "Y_train shape: (2, 136)\n",
      "X_test shape: (7500, 24)\n",
      "Y_test shape: (2, 24)\n"
     ]
    }
   ],
   "source": [
    "#flattening the images of trainig and test set\n",
    "x_train_flatten = x_train.reshape(x_train.shape[0], -1).T\n",
    "x_test_flatten = x_test.reshape(x_test.shape[0], -1).T\n",
    "#normalizing image vectors\n",
    "x_train = x_train_flatten/255.\n",
    "x_test = x_test_flatten/255.\n",
    "# Converting training and test labels to one hot matrices\n",
    "\n",
    "y_train = convert_to_one_hot(y_train, 2)\n",
    "y_test = convert_to_one_hot(y_test, 2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print (\"number of training examples = \" + str(x_train.shape[1]))\n",
    "print (\"number of test examples = \" + str(x_test.shape[1]))\n",
    "print (\"X_train shape: \" + str(x_train.shape))\n",
    "print (\"Y_train shape: \" + str(y_train.shape))\n",
    "print (\"X_test shape: \" + str(x_test.shape))\n",
    "print (\"Y_test shape: \" + str(y_test.shape))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating tensors for x and y\n",
    "x_train_tensor = tf.convert_to_tensor(x_train,dtype = tf.float32)\n",
    "y_train_tensor = tf.convert_to_tensor(y_train,dtype = tf.float32)\n",
    "x_test_tensor = tf.convert_to_tensor(x_test,dtype = tf.float32)\n",
    "y_test_tensor = tf.convert_to_tensor(y_test,dtype = tf.float32)\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing the parameters\n",
    "def initialize_parameters():\n",
    "    parameters = {}\n",
    "    initializer_W = tf.initializers.GlorotUniform()\n",
    "    initializer_b = tf.zeros_initializer()\n",
    "    \n",
    "    \n",
    "    \n",
    "    W1 = tf.Variable(initializer_W((50,7500)))\n",
    "    b1 = tf.Variable(initializer_b(shape=[50,1], dtype=tf.float32))\n",
    "    W2 = tf.Variable(initializer_W((50,50)))\n",
    "    b2 = tf.Variable(initializer_b(shape=[50,1], dtype=tf.float32))\n",
    "    W3 = tf.Variable(initializer_W((2,50)))\n",
    "    b3 = tf.Variable(initializer_b(shape=[2,1], dtype=tf.float32))\n",
    "    parameters = {\"W1\": W1,\"b1\": b1,\"W2\": W2,\"b2\": b2,\"W3\": W3,\"b3\": b3}\n",
    "    return parameters\n",
    "parameters = initialize_parameters()\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "                  \n",
    "                  \n",
    "                  \n",
    "                  \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "                  \n",
    "                  \n",
    "                  \n",
    "                  \n",
    "                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#forward propagation\n",
    "parameters = initialize_parameters()\n",
    "def forward_propagation(X, parameters):\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3']\n",
    "    \n",
    "    Z1 = tf.add(tf.matmul(W1, X),b1)                                              \n",
    "    A1 = tf.nn.relu(Z1)                                             \n",
    "    Z2 = tf.add(tf.matmul(W2, A1),b2)                                            \n",
    "    A2 = tf.nn.relu(Z2)                                            \n",
    "    Z3 = tf.add(tf.matmul(W3, A2),b3)\n",
    "    \n",
    "    return Z3\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.31624144 0.23865789 0.14560528 0.33699074 0.13573597 0.2249802\n",
      "  0.35485405 0.13464561 0.32395554 0.29480907 0.14765808 0.32055733\n",
      "  0.25341153 0.40324405 0.2700525  0.2547303  0.35427865 0.13972084\n",
      "  0.15860713 0.14546973 0.39963022 0.22332717 0.25144726 0.1889072\n",
      "  0.07187275 0.23750955 0.25293353 0.2194767  0.12122935 0.26195285\n",
      "  0.21252178 0.2026538  0.34297943 0.36021978 0.29200494 0.23043546\n",
      "  0.4038861  0.26513332 0.14840394 0.15012276 0.18621884 0.34892493\n",
      "  0.28349057 0.22731304 0.2913029  0.14943887 0.23109034 0.18033606\n",
      "  0.13989352 0.16594057 0.18240398 0.25879377 0.16190633 0.35179287\n",
      "  0.3355649  0.12482838 0.3944646  0.19869807 0.34960347 0.08800005\n",
      "  0.22270766 0.17251492 0.16271493 0.1656837  0.09892006 0.33778197\n",
      "  0.19037327 0.21005037 0.08371142 0.11731995 0.29144984 0.32131982\n",
      "  0.16339639 0.3198749  0.14310478 0.2794101  0.23050573 0.1481039\n",
      "  0.26611924 0.2742545  0.2925599  0.05467377 0.04059188 0.20415848\n",
      "  0.328789   0.11096585 0.12133324 0.18063672 0.20575625 0.34856302\n",
      "  0.18307504 0.37136793 0.3458609  0.2961025  0.4393329  0.4230774\n",
      "  0.12506607 0.1859485  0.23008564 0.21085098 0.19997332 0.19682333\n",
      "  0.22577493 0.24386321 0.12870923 0.12416255 0.2762072  0.2131186\n",
      "  0.40539336 0.3396865  0.21906416 0.33038732 0.30311304 0.17809622\n",
      "  0.3396161  0.2333175  0.3548118  0.23246716 0.14219593 0.1794084\n",
      "  0.23194742 0.2571959  0.19300953 0.2107113  0.0496319  0.21653923\n",
      "  0.33634767 0.37434784 0.34550515 0.18237755 0.27955505 0.23680475\n",
      "  0.29492873 0.3795428  0.34761083 0.20011552]\n",
      " [0.18416801 0.2765009  0.21832436 0.23416261 0.06890656 0.22687584\n",
      "  0.15475431 0.08879009 0.21030861 0.18294758 0.19942911 0.26730528\n",
      "  0.15351476 0.20974952 0.19185847 0.14676899 0.26111734 0.06891325\n",
      "  0.18226007 0.14897135 0.26005036 0.2151948  0.20423113 0.201884\n",
      "  0.16980797 0.13980249 0.21668726 0.1980716  0.09035283 0.21736604\n",
      "  0.18832958 0.09467623 0.15429695 0.32555342 0.24029674 0.20722693\n",
      "  0.16005728 0.24166785 0.13156065 0.21521203 0.18440062 0.3086235\n",
      "  0.16071004 0.15458022 0.11157402 0.16263609 0.22053154 0.15916021\n",
      "  0.08274746 0.14751065 0.20698729 0.17437968 0.08623996 0.18322802\n",
      "  0.20349607 0.07840505 0.1762761  0.22419316 0.29917327 0.09984201\n",
      "  0.17380603 0.09356827 0.10017392 0.10778767 0.06194167 0.19459334\n",
      "  0.18519214 0.18159778 0.1706256  0.11494899 0.15261483 0.19571725\n",
      "  0.15351106 0.17787512 0.1857327  0.20360026 0.12144944 0.23410966\n",
      "  0.2433474  0.15846668 0.2252294  0.10792309 0.10708414 0.22194742\n",
      "  0.20203277 0.02809474 0.0864439  0.12165038 0.12027641 0.2523166\n",
      "  0.1702718  0.3193072  0.18033972 0.24765164 0.15376215 0.21326944\n",
      "  0.03290281 0.09932765 0.14049512 0.19718269 0.2336401  0.13449225\n",
      "  0.14441048 0.18464096 0.13218464 0.00133196 0.15619321 0.13562375\n",
      "  0.26860946 0.18059626 0.16347682 0.19755742 0.16902967 0.21635425\n",
      "  0.1812022  0.15290783 0.23652604 0.2036945  0.1225858  0.2166642\n",
      "  0.14641336 0.15685695 0.19463436 0.1415903  0.09898447 0.18638594\n",
      "  0.22177765 0.196193   0.23956643 0.14170639 0.11564997 0.18783106\n",
      "  0.24373388 0.23148277 0.19295575 0.20832147]], shape=(2, 136), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_parameters()\n",
    "z3 = forward_propagation(x_train_tensor, parameters)\n",
    "print(z3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#computing cost \n",
    "def compute_cost(Z3, Y):\n",
    "     \n",
    "    logits = tf.transpose(Z3)\n",
    "    labels = tf.transpose(Y)\n",
    "    \n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = labels ))\n",
    "   \n",
    "    return cost\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining model\n",
    "def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.0001,num_passes = 1500,print_cost = True):\n",
    "    (n_x, m) = X_train.shape\n",
    "    n_y = Y_train.shape[0]\n",
    "    costs = []\n",
    "    #initializing parameters\n",
    "    parameters = initialize_parameters()\n",
    "    for i in range(num_passes):\n",
    "        #forward propagation\n",
    "        Z3 = forward_propagation(X_train, parameters)\n",
    "        #computing cost \n",
    "        cost = compute_cost(Z3, Y_train)\n",
    "        #backprop/gradient descent with adam optimizer\n",
    "        optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "        #printing cost after 100th iteraton\n",
    "        if print_cost == True and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost == True and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "    #plotting the costs graph\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per hundreds)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    return parameters \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "`loss` passed to Optimizer.compute_gradients should be a function when eager execution is enabled.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-26ed2892262d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mparameters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-17-707368edfc41>\u001b[0m in \u001b[0;36mmodel\u001b[1;34m(X_train, Y_train, X_test, Y_test, learning_rate, num_passes, print_cost)\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;31m#backprop/gradient descent with adam optimizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax_cross_entropy_with_logits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[1;31m#printing cost after 100th iteraton\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mprint_cost\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mTrue\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m100\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\anaconda\\envs\\tensor-flow\\lib\\site-packages\\tensorflow_core\\python\\training\\optimizer.py\u001b[0m in \u001b[0;36mminimize\u001b[1;34m(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\u001b[0m\n\u001b[0;32m    401\u001b[0m         \u001b[0maggregation_method\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maggregation_method\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    402\u001b[0m         \u001b[0mcolocate_gradients_with_ops\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolocate_gradients_with_ops\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 403\u001b[1;33m         grad_loss=grad_loss)\n\u001b[0m\u001b[0;32m    404\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    405\u001b[0m     \u001b[0mvars_with_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mv\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mg\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\anaconda\\envs\\tensor-flow\\lib\\site-packages\\tensorflow_core\\python\\training\\optimizer.py\u001b[0m in \u001b[0;36mcompute_gradients\u001b[1;34m(self, loss, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, grad_loss)\u001b[0m\n\u001b[0;32m    479\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    480\u001b[0m       raise RuntimeError(\n\u001b[1;32m--> 481\u001b[1;33m           \u001b[1;34m\"`loss` passed to Optimizer.compute_gradients should \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    482\u001b[0m           \"be a function when eager execution is enabled.\")\n\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: `loss` passed to Optimizer.compute_gradients should be a function when eager execution is enabled."
     ]
    }
   ],
   "source": [
    "parameters = model(x_train_tensor, y_train_tensor, x_test_tensor, y_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
